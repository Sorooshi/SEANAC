{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import networkx  as nx\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import copy, deepcopy\n",
    "from collections import OrderedDict, defaultdict\n",
    "# import simultaneous_anomalous_clustering as sanc\n",
    "from sklearn.metrics.cluster import contingency_matrix\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "np.set_printoptions(suppress=True, linewidth=150, precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_ground_truth(ground_truth):\n",
    "    \n",
    "    labels_true, labels_true_indices = [], []\n",
    "    for k, v in ground_truth.items():\n",
    "        for vv in v:\n",
    "            labels_true.append(int(k)+1)\n",
    "            labels_true_indices.append(vv)\n",
    "            \n",
    "    return labels_true, labels_true_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data for small-size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('../data', 'MC(1000, 10, 15).pickle'), 'rb') as fp:\n",
    "    SAN = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting: (0.7, 0.3, 0.7)\n",
      "setting: (0.7, 0.3, 0.9)\n",
      "setting: (0.7, 0.6, 0.7)\n",
      "setting: (0.7, 0.6, 0.9)\n",
      "setting: (0.9, 0.3, 0.7)\n",
      "setting: (0.9, 0.3, 0.9)\n",
      "setting: (0.9, 0.6, 0.7)\n",
      "setting: (0.9, 0.6, 0.9)\n"
     ]
    }
   ],
   "source": [
    "SETTINGS = []\n",
    "for setting, repeats in SAN.items():\n",
    "    SETTINGS.append(setting)\n",
    "    print(\"setting:\", setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing data for CESNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# key = 0\n",
    "# name = 'MC'\n",
    "\n",
    "# for setting, repeats in SAN.items():\n",
    "#     print(\"setting:\", setting)\n",
    "            \n",
    "#     for repeat, matrices in repeats.items():\n",
    "#         print(\"repeat:\", repeat)\n",
    "\n",
    "#         GT = matrices['GT']\n",
    "\n",
    "#         Yin  = matrices['Y']\n",
    "#         N, V = Yin.shape\n",
    "\n",
    "#         Ynin = matrices['Yn']\n",
    "\n",
    "#         Pin  = matrices['P']\n",
    "#         Np, Vp = Pin.shape\n",
    "\n",
    "#         # This section is not needed here for the conversion\n",
    "#         k = 1\n",
    "#         interval = 1\n",
    "#         labels_true_sorted_final, labels_true_indices = [], []\n",
    "#         for v in GT:\n",
    "#             tmp_indices = []\n",
    "#             for vv in range(v):\n",
    "#                 labels_true_sorted_final.append(str(k))\n",
    "#                 tmp_indices.append(interval+vv)\n",
    "\n",
    "#             k += 1\n",
    "#             interval += v\n",
    "#             labels_true_indices += tmp_indices\n",
    "\n",
    "#         # preparing data for CESNA\n",
    "#         Yc_unique = np.zeros([N, V])\n",
    "#         interval = 0\n",
    "\n",
    "#         for v in range(V):\n",
    "#             for i in range(N):\n",
    "#                 Yc_unique[i, v] = Yin[i, v] + interval\n",
    "#             interval += max(list(set(Yin[:, v])))\n",
    "\n",
    "\n",
    "#         test_feature = []\n",
    "#         key += 1\n",
    "# #         key = [str(i) for i in setting]\n",
    "# #         key = ''.join(key)\n",
    "# #         key += str(repeat)\n",
    "\n",
    "#         with open (os.path.join(\"cesna_medium_data/\", str(key) + \".nodefeat\"), \"w\") as fp:\n",
    "#             for v in range(V):\n",
    "#                 for i in range(N):\n",
    "#                     fp.write(str(i+1))\n",
    "#                     fp.write(\"\\t\")\n",
    "#                     fp.write(str(int(Yc_unique[i, v])))\n",
    "#                     fp.write(\"\\n\")\n",
    "#                     test_feature.append((str(i+1), str(int(Yc_unique[i, v]))))\n",
    "\n",
    "#         diff_features = []\n",
    "#         for v in range(1, V):\n",
    "#             for i in range(N):\n",
    "#                 if not (str(i+1) , str(int(Yc_unique[i, v]))) in test_feature:\n",
    "#                     diff_features.append((str(i+1) , str(int(Yc_unique[i, v]))))\n",
    "\n",
    "#         print(\"  \")\n",
    "#         print(\"diff features:\", diff_features, len(diff_features))\n",
    "\n",
    "#         test_nets = []\n",
    "#         with open (os.path.join(\"cesna_medium_data/\", str(key) + \".edges\"), \"w\") as fp:\n",
    "#             for i in range(Np):\n",
    "#                 for j in range(Np):\n",
    "#                     if Pin[i, j] != 0 or Pin[j, i] != 0:\n",
    "#                         fp.write(str(i+1))\n",
    "#                         fp.write(\"\\t\")\n",
    "#                         fp.write(str(j+1))\n",
    "#                         fp.write(\"\\n\")\n",
    "#                         test_nets.append((str(i+1), str(j+1)))\n",
    "#         #                test_nets.append((str(j+1), str(i+1)))                                                                                       \n",
    "\n",
    "# #             diff_nets = []\n",
    "# #             for i in range(Np):\n",
    "# #                 print(\"i:\", i)\n",
    "# #                 for j in range(Np):\n",
    "# #                     if Pin[i, j] != 0:\n",
    "# #                         if not (str(i+1), str(j+1)) in test_nets:\n",
    "# #                             diff_nets.append((str(i+1), str(j+1)))\n",
    "\n",
    "# #             print(\"test_nets:\", len(test_nets))\n",
    "# #             print(\"diff_nets:\", diff_nets, len(diff_nets))\n",
    "#         print(\"done!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating CESNA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the saved result and other post processing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI_CESNA = {SETTINGS[j//10]:{} for j in range(80)}  # 80 data sets \n",
    "NMI_CESNA = {SETTINGS[j//10]:{} for j in range(80)}  # 80 data sets \n",
    "AMI_CESNA = {SETTINGS[j//10]:{} for j in range(80)}  # 80 data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of applying CESNA:\n",
      "\t   p   q  a/e \t   ARI        NMI\n",
      " \t  \t  \t  Ave  std  Ave  std\n",
      "(0.7, 0.3, 0.7) 0.715 0.128 0.838 0.061\n",
      "(0.7, 0.3, 0.9) 0.764 0.068 0.886 0.034\n",
      "(0.7, 0.6, 0.7) 0.016 0.008 0.251 0.023\n",
      "(0.7, 0.6, 0.9) 0.060 0.024 0.352 0.064\n",
      "(0.9, 0.3, 0.7) 0.849 0.076 0.921 0.026\n",
      "(0.9, 0.3, 0.9) 0.894 0.053 0.939 0.031\n",
      "(0.9, 0.6, 0.7) 0.474 0.089 0.661 0.059\n",
      "(0.9, 0.6, 0.9) 0.632 0.058 0.752 0.039\n"
     ]
    }
   ],
   "source": [
    "print(\"results of applying CESNA:\")\n",
    "print(\"\\t\", \"  p\", \"  q\", \" a/e\", \"\\t\", \"  ARI     \", \"  NMI\",)\n",
    "print(\" \\t\", \" \\t\", \" \\t\", \" Ave\", \" std\", \" Ave\", \" std\")\n",
    "\n",
    "\n",
    "ari_cesna, nmi_cesna, ami_cesna = [], [], []\n",
    "for i in range(81):  \n",
    "    \n",
    "    if i+1 <= 80:\n",
    "        \n",
    "        repeat  = (i %10) + 1\n",
    "        setting = i // 10\n",
    "\n",
    "        # load the saved result\n",
    "        with open(\"/home/Soroosh/snap/examples/cesna/results_medium/medium_\" + str(i+1)+ \"cmtyvv.txt\", 'r') as fp:\n",
    "            cesna = fp.readlines()\n",
    "        \n",
    "            \n",
    "        GT = SAN[SETTINGS[setting]][repeat-1]['GT']\n",
    "\n",
    "        # This section is not needed here for the conversion\n",
    "        k = 1\n",
    "        interval = 1\n",
    "        labels_true_final_cesna, labels_true_indices = [], []\n",
    "        for v in GT:\n",
    "            tmp_indices = []\n",
    "            for vv in range(v):\n",
    "                labels_true_final_cesna.append(str(k))\n",
    "                tmp_indices.append(interval+vv)\n",
    "\n",
    "            k += 1\n",
    "            interval += v\n",
    "            labels_true_indices += tmp_indices\n",
    "        \n",
    "        N = len(labels_true_final_cesna)\n",
    "\n",
    "        # flattening the result   \n",
    "        cesna_indices = []\n",
    "        for i in cesna:\n",
    "            cesna_indices.append([int(ii) for ii in i.split()])\n",
    "        \n",
    "        cesna_labels = np.zeros([N])\n",
    "        label = 1\n",
    "        for lst in cesna_indices:\n",
    "            for l in lst:\n",
    "                cesna_labels[l-1] = label\n",
    "            label += 1\n",
    "\n",
    "\n",
    "        ari_cesna.append(metrics.adjusted_rand_score(labels_true=labels_true_final_cesna, labels_pred=cesna_labels))\n",
    "        nmi_cesna.append(metrics.normalized_mutual_info_score(labels_true=labels_true_final_cesna, labels_pred=cesna_labels))\n",
    "        ami_cesna.append(metrics.adjusted_mutual_info_score(labels_true=labels_true_final_cesna, labels_pred=cesna_labels))\n",
    "\n",
    "        if repeat == 10:\n",
    "\n",
    "            ARI_CESNA[SETTINGS[setting]][str(repeat)] = ari_cesna\n",
    "            NMI_CESNA[SETTINGS[setting]][str(repeat)] = nmi_cesna\n",
    "            AMI_CESNA[SETTINGS[setting]][str(repeat)] = ami_cesna\n",
    "\n",
    "            print( SETTINGS[setting],\n",
    "                  \"%.3f\" % np.mean(np.asarray(ari_cesna)),\n",
    "                  \"%.3f\" % np.std(np.asarray(ari_cesna)),\n",
    "                  '%0.3f' % np.mean(np.asarray(nmi_cesna)),\n",
    "                  '%0.3f' % np.std(np.asarray(nmi_cesna))   \n",
    "                 )\n",
    "            \n",
    "\n",
    "            ari_cesna, nmi_cesna, ami_cesna = [], [], []    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing data for SIAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# key = 0\n",
    "# index = 0\n",
    "# for setting, repeats in SAN.items():\n",
    "    \n",
    "# #     print(\"setting:\", setting)\n",
    "#     for repeat, matrices in repeats.items():\n",
    "# #         print(\"repeat:\", repeat)\n",
    "        \n",
    "#         GT = matrices['GT']\n",
    "        \n",
    "#         Yin  = matrices['Y']\n",
    "#         N, V = Yin.shape\n",
    "        \n",
    "#         Ynin = matrices['Yn']\n",
    "        \n",
    "#         Pin  = matrices['P']\n",
    "#         Np, Vp = Pin.shape\n",
    "        \n",
    "#         array2list = []\n",
    "#         for i in range(N):\n",
    "#             array2list.append(list(Yin[i, :]))\n",
    "            \n",
    "#         unique_features = []\n",
    "#         for i in array2list:\n",
    "#             if not i in unique_features:\n",
    "#                 unique_features.append(i)\n",
    "\n",
    "# #         print(\"unique_features:\", unique_features)\n",
    "        \n",
    "#         labels_comb_man = [\"meta\" + str(i+1) for i in range(len(unique_features))]\n",
    "# #         print(len(labels_comb_man))\n",
    "        \n",
    "#         Ggml = nx.from_numpy_array(Pin)\n",
    "\n",
    "#         attributes_dict = {}\n",
    "                \n",
    "#         label = 1\n",
    "#         labels_true = []\n",
    "#         for uf in unique_features:  #UniqueFeatures\n",
    "#             tmp_labels = []\n",
    "#             for i in range(N):\n",
    "#                 if uf == array2list[i]:\n",
    "#                     attributes_dict[i] ={}\n",
    "#                     key = labels_comb_man[label-1]\n",
    "#                     attributes_dict[i][key] = key\n",
    "#                     tmp_labels.append(label)\n",
    "#             label += 1\n",
    "        \n",
    "#         index += 1\n",
    "#         nx.set_node_attributes(Ggml, attributes_dict)\n",
    "#         nx.write_gml(G=Ggml, path=\"sian_medium_data_tmp/medium_\" + str(index) + \"_SIAN.gml\")\n",
    "        \n",
    "#         with open (\"sian_medium_data_tmp/medium_\"+ str(index) +\"_SIAN.gml\", 'r') as fp:\n",
    "#             GML = fp.readlines()\n",
    "        \n",
    "#         with open (\"sian_medium_data/medium_\" + str(index) +\".gml\", 'w') as fp:\n",
    "#             for i in range(len(GML)):\n",
    "#                 if i ==3:\n",
    "#                     fp.write(GML[0])\n",
    "#                     fp.write(GML[1])\n",
    "#                     fp.write(GML[2])\n",
    "#                     fp.write(\"    \")\n",
    "#                     fp.write(GML[3].split()[0])\n",
    "#                     fp.write(\" \")\n",
    "#                     fp.write(GML[4].split()[1])\n",
    "#                     fp.write(\"\\n\")\n",
    "#                     fp.write(GML[5])\n",
    "#                 elif \"label\" in GML[i] and i!=3:\n",
    "#                     fp.write(GML[i-2])\n",
    "#                     fp.write(GML[i-1])\n",
    "#                     fp.write(\"    \")\n",
    "#                     fp.write(GML[i].split()[0])\n",
    "#                     fp.write(\" \")\n",
    "#                     fp.write(GML[i+1].split()[1])\n",
    "#                     fp.write(\"\\n\")\n",
    "#                     fp.write(GML[i+2])\n",
    "#                 elif \"edge\" in GML[i]:\n",
    "#                     fp.write(GML[i])\n",
    "#                     fp.write(GML[i+1])\n",
    "#                     fp.write(GML[i+2])\n",
    "#                     fp.write(GML[i+4])\n",
    "                    \n",
    "                    \n",
    "# print(\"Conversion is Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the SIAN's results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI_SIAN = {SETTINGS[j//10]:{} for j in range(80)}  # 80 data sets \n",
    "NMI_SIAN = {SETTINGS[j//10]:{} for j in range(80)}  # 80 data sets \n",
    "AMI_SIAN = {SETTINGS[j//10]:{} for j in range(80)}  # 80 data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results of applying SIAN:\n",
      "\t   p   q  a/e \t   ARI        NMI\n",
      " \t  \t  \t  Ave  std  Ave  std\n",
      "(0.7, 0.3, 0.7) 0.000 0.000 -0.000 0.000\n",
      "(0.7, 0.3, 0.9) 0.026 0.077 0.059 0.178\n",
      "(0.7, 0.6, 0.7) 0.000 0.000 -0.000 0.000\n",
      "(0.7, 0.6, 0.9) 0.000 0.000 -0.000 0.000\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/Soroosh/Newman_2016/Newman_Clauset_code/sian_medium_results/medium_result48.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4787123ca788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mrepeat\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0msetting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/Soroosh/Newman_2016/Newman_Clauset_code/sian_medium_results/medium_result\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mSIAN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/Soroosh/Newman_2016/Newman_Clauset_code/sian_medium_results/medium_result48.txt'"
     ]
    }
   ],
   "source": [
    "print(\"results of applying SIAN:\")\n",
    "print(\"\\t\", \"  p\", \"  q\", \" a/e\", \"\\t\", \"  ARI     \", \"  NMI\",)\n",
    "print(\" \\t\", \" \\t\", \" \\t\", \" Ave\", \" std\", \" Ave\", \" std\")\n",
    "\n",
    "# dataset_in_which_SIAN_does_not_converge = [2, 6, 10, 21, 29, 42, 44, 47, 48, 49, 61, 62, 65, 66, 67, 68, 69] \n",
    "dataset_in_which_SIAN_produces_memory_error = []\n",
    "N = 1000\n",
    "ari_sian, nmi_sian, ami_sian = [], [], []\n",
    "\n",
    "for i in range(81):  #81\n",
    "    \n",
    "    # load the saved result\n",
    "    if i+1 <= 80:  # and not i+1 in dataset_in_which_SIAN_does_not_converge:\n",
    "        repeat  = (i %10) + 1\n",
    "        setting = i // 10\n",
    "        with open(\"/home/Soroosh/Newman_2016/Newman_Clauset_code/sian_medium_results/medium_result\" + str(i+1)+ \".txt\", 'rb') as fp:\n",
    "            SIAN = fp.readlines()\n",
    "        \n",
    "        sian_labels = []\n",
    "        if len(SIAN) == N:  # number of nodes = number of line in sians output\n",
    "            for line in range(len(SIAN)):\n",
    "                tmp = SIAN[line].split()\n",
    "                prob = [float(tmp[t]) for t in range(len(tmp))  if t > 1]\n",
    "                sian_labels.append(np.argmax(np.asarray(prob)))\n",
    "        else:\n",
    "            dataset_in_which_SIAN_produces_memory_error.append(i+1)\n",
    "            sian_labels += ['0' for i in range(N)]\n",
    "            \n",
    "            \n",
    "        GT = SAN[SETTINGS[setting]][repeat-1]['GT']\n",
    "        k = 1\n",
    "        interval = 1\n",
    "        labels_true_final_sian, labels_true_indices = [], []\n",
    "        for v in GT:\n",
    "            tmp_indices = []\n",
    "            for vv in range(v):\n",
    "                labels_true_final_sian.append(str(k))\n",
    "                tmp_indices.append(interval+vv)\n",
    "\n",
    "            k += 1\n",
    "            interval += v\n",
    "            labels_true_indices += tmp_indices\n",
    "\n",
    "        \n",
    "        ari_sian.append(metrics.adjusted_rand_score(labels_true=labels_true_final_sian, labels_pred=sian_labels))\n",
    "        nmi_sian.append(metrics.normalized_mutual_info_score(labels_true=labels_true_final_sian, labels_pred=sian_labels, average_method='arithmetic'))\n",
    "        ami_sian.append(metrics.adjusted_mutual_info_score(labels_true=labels_true_final_sian, labels_pred=sian_labels,))\n",
    "\n",
    "        if repeat == 10:\n",
    "\n",
    "            ARI_SIAN[SETTINGS[setting]][str(repeat)] = ari_sian\n",
    "            NMI_SIAN[SETTINGS[setting]][str(repeat)] = nmi_sian\n",
    "            AMI_SIAN[SETTINGS[setting]][str(repeat)] = ami_sian\n",
    "            \n",
    "            print( SETTINGS[setting],\n",
    "                  \"%.3f\" % np.mean(np.asarray(ari_sian)),\n",
    "                  \"%.3f\" % np.std(np.asarray(ari_sian)),\n",
    "                  '%0.3f' % np.mean(np.asarray(nmi_sian)),\n",
    "                  '%0.3f' % np.std(np.asarray(nmi_sian))   \n",
    "                 )\n",
    "\n",
    "            ari_sian, nmi_sian, ami_sian = [], [], []    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
